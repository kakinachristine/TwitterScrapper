{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                        date                   id  \\\n0  2023-04-12 08:05:43+00:00  1646062024852668418   \n1  2023-04-12 08:05:34+00:00  1646061984264355841   \n2  2023-04-12 08:04:49+00:00  1646061799136174082   \n3  2023-04-12 08:04:28+00:00  1646061709746896897   \n4  2023-04-12 08:04:04+00:00  1646061607829618688   \n5  2023-04-12 08:04:02+00:00  1646061599852048385   \n6  2023-04-12 08:03:58+00:00  1646061584991567873   \n7  2023-04-12 08:03:04+00:00  1646061355621928960   \n8  2023-04-12 08:02:57+00:00  1646061327285116929   \n9  2023-04-12 08:02:54+00:00  1646061316057219073   \n\n                                             content        username  \\\n0  風の強い日でした\\n#セルフポートレート #nana_camera \\n#中性俳優 #モデル...   irodor1midori   \n1  #Pokekara 友の多重ハモリコーラスで盛り上げてくれています。 #中山美穂『　#色ホワ...         haoyu52   \n2  運動すると頭を空に出来るから気になってたことを整理するきっかけにもなる👏👏\\n終わった後の達...     ken19891016   \n3  #gay #LaMasDraga #boy #gaykiss #kep1er #LESSER...       aaasael27   \n4  Sen ancak @RTErdogan'ın tırnağından özür diler...  marjinal_koylu   \n5  Hora de descansar estas nalgas jajaja #gay htt...       pepecol69   \n6        Pussy! \\n\\n#LGBTQIA https://t.co/Dr0QpVKr0T    JustChillieh   \n7  La Procura di #Padova ha chiesto al Comune gli...   BiribissiBlog   \n8  Research is key to every #Crypto investment🗣️ ...        RBIF_USA   \n9  髪伸びました。\\nL/20代/東北\\n好きなものは、きなこ餅せんべい\\n嫌いなものは信号と横...       izumi_Si3   \n\n   likeCount  retweetCount               location  \\\n0          0             0                    NaN   \n1          0             0                  日本 東京   \n2          0             0                    NaN   \n3          0             0         Ahome, Sinaloa   \n4          0             0                    NaN   \n5          0             0         Colima, México   \n6          1             0  Palm Beach County, FL   \n7          0             0       Sanremo, Liguria   \n8          2             0            Buffalo, NY   \n9          0             0                    NaN   \n\n                                             hashtag  \\\n0  ['セルフポートレート', 'nana_camera', '中性俳優', 'モデル', 'L...   \n1  ['Pokekara', '中山美穂', '色ホワイトブレンド', 'G組合員', 'Gay...   \n2  ['悩み相談', '悩み聞いて', 'カウンセラー', 'ケアマネ', 'LGBTQ', '...   \n3  ['gay', 'LaMasDraga', 'boy', 'gaykiss', 'kep1e...   \n4                   ['LGBT', 'ErdoğanYineKazanacak']   \n5                                            ['gay']   \n6                                        ['LGBTQIA']   \n7                          ['Padova', 'gay', 'ANSA']   \n8  ['Crypto', 'DYOR', 'Finance', 'RoboInu', 'Bank...   \n9  ['LGBTQ', 'LGBTさんと繋がりたい', 'セクマイ', 'セクマイさんと繋がりた...   \n\n                                              source  \n0  <a href=\"http://twitter.com/download/android\" ...  \n1  <a href=\"http://www.pokekara.com\" rel=\"nofollo...  \n2  <a href=\"http://twitter.com/download/iphone\" r...  \n3  <a href=\"http://twitter.com/download/iphone\" r...  \n4  <a href=\"http://twitter.com/download/android\" ...  \n5  <a href=\"http://twitter.com/download/android\" ...  \n6  <a href=\"http://twitter.com/download/iphone\" r...  \n7  <a href=\"http://twitter.com/download/android\" ...  \n8  <a href=\"http://twitter.com/download/iphone\" r...  \n9  <a href=\"http://twitter.com/download/android\" ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>id</th>\n      <th>content</th>\n      <th>username</th>\n      <th>likeCount</th>\n      <th>retweetCount</th>\n      <th>location</th>\n      <th>hashtag</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2023-04-12 08:05:43+00:00</td>\n      <td>1646062024852668418</td>\n      <td>風の強い日でした\\n#セルフポートレート #nana_camera \\n#中性俳優 #モデル...</td>\n      <td>irodor1midori</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>['セルフポートレート', 'nana_camera', '中性俳優', 'モデル', 'L...</td>\n      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2023-04-12 08:05:34+00:00</td>\n      <td>1646061984264355841</td>\n      <td>#Pokekara 友の多重ハモリコーラスで盛り上げてくれています。 #中山美穂『　#色ホワ...</td>\n      <td>haoyu52</td>\n      <td>0</td>\n      <td>0</td>\n      <td>日本 東京</td>\n      <td>['Pokekara', '中山美穂', '色ホワイトブレンド', 'G組合員', 'Gay...</td>\n      <td>&lt;a href=\"http://www.pokekara.com\" rel=\"nofollo...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023-04-12 08:04:49+00:00</td>\n      <td>1646061799136174082</td>\n      <td>運動すると頭を空に出来るから気になってたことを整理するきっかけにもなる👏👏\\n終わった後の達...</td>\n      <td>ken19891016</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>['悩み相談', '悩み聞いて', 'カウンセラー', 'ケアマネ', 'LGBTQ', '...</td>\n      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-04-12 08:04:28+00:00</td>\n      <td>1646061709746896897</td>\n      <td>#gay #LaMasDraga #boy #gaykiss #kep1er #LESSER...</td>\n      <td>aaasael27</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Ahome, Sinaloa</td>\n      <td>['gay', 'LaMasDraga', 'boy', 'gaykiss', 'kep1e...</td>\n      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023-04-12 08:04:04+00:00</td>\n      <td>1646061607829618688</td>\n      <td>Sen ancak @RTErdogan'ın tırnağından özür diler...</td>\n      <td>marjinal_koylu</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>['LGBT', 'ErdoğanYineKazanacak']</td>\n      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2023-04-12 08:04:02+00:00</td>\n      <td>1646061599852048385</td>\n      <td>Hora de descansar estas nalgas jajaja #gay htt...</td>\n      <td>pepecol69</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Colima, México</td>\n      <td>['gay']</td>\n      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2023-04-12 08:03:58+00:00</td>\n      <td>1646061584991567873</td>\n      <td>Pussy! \\n\\n#LGBTQIA https://t.co/Dr0QpVKr0T</td>\n      <td>JustChillieh</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Palm Beach County, FL</td>\n      <td>['LGBTQIA']</td>\n      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2023-04-12 08:03:04+00:00</td>\n      <td>1646061355621928960</td>\n      <td>La Procura di #Padova ha chiesto al Comune gli...</td>\n      <td>BiribissiBlog</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Sanremo, Liguria</td>\n      <td>['Padova', 'gay', 'ANSA']</td>\n      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2023-04-12 08:02:57+00:00</td>\n      <td>1646061327285116929</td>\n      <td>Research is key to every #Crypto investment🗣️ ...</td>\n      <td>RBIF_USA</td>\n      <td>2</td>\n      <td>0</td>\n      <td>Buffalo, NY</td>\n      <td>['Crypto', 'DYOR', 'Finance', 'RoboInu', 'Bank...</td>\n      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2023-04-12 08:02:54+00:00</td>\n      <td>1646061316057219073</td>\n      <td>髪伸びました。\\nL/20代/東北\\n好きなものは、きなこ餅せんべい\\n嫌いなものは信号と横...</td>\n      <td>izumi_Si3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>['LGBTQ', 'LGBTさんと繋がりたい', 'セクマイ', 'セクマイさんと繋がりた...</td>\n      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Read in the data\n",
    "tweets_df = pd.read_csv(r\"C:\\Users\\user\\PycharmProjects\\TwitterYouths\\lgbtq_hashtags.csv\")\n",
    "tweets_df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "tweets_df.drop_duplicates(subset=\"content\",keep=\"first\",inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Remove symbols, numbers, and URL links\n",
    "tweets_df['content'] = tweets_df['content'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "tweets_df['content'] = tweets_df['content'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "tweets_df['content'] = tweets_df['content'].apply(lambda x: re.sub(r'https?://\\S+', '', x))\n",
    "tweets_df['content'] = tweets_df['content'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "tweets_df['content'] = tweets_df['content'].apply(lambda x: re.sub(r'#\\S+', '', x))\n",
    "tweets_df[\"content\"]= tweets_df['content'].replace(r'\\r', ' ', regex=True)\n",
    "tweets_df[\"content\"]= tweets_df['content'].replace(r'\\n', ' ', regex=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "0        風の強い日でした セルフポートレート nana_camera  中性俳優 モデル LGBTQ...\n1        Pokekara 友の多重ハモリコーラスで盛り上げてくれています 中山美穂　色ホワイトブレン...\n2        運動すると頭を空に出来るから気になってたことを整理するきっかけにもなる 終わった後の達成感な...\n3        gay LaMasDraga boy gaykiss keper LESSERAFIM Ne...\n4        Sen ancak RTErdoganın tırnağından özür dilersi...\n                               ...                        \n14997    Guten Morgen ihr lieben  wünsche euch schonmal...\n14998    Its all happening in  minutes You all know whe...\n14999    Not sure why everyone is so worried about Tran...\n15000    elio_vito Libero_official alesallusti Pan per ...\n15001    Open VcsBo pijat  gay gayindo GayIndonesia gay...\nName: content, Length: 14691, dtype: object"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"content\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Remove emojis\n",
    "def remove_emojis(content):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', content)\n",
    "\n",
    "tweets_df['content'] = tweets_df['content'].apply(remove_emojis)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mstopwords\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/stopwords\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 84\u001B[0m     root \u001B[38;5;241m=\u001B[39m \u001B[43mnltk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubdir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mzip_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\data.py:583\u001B[0m, in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    582\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 583\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[1;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mstopwords\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/stopwords.zip/stopwords/\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [7], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Remove stop words and lemmatize\u001B[39;00m\n\u001B[0;32m      2\u001B[0m nltk\u001B[38;5;241m.\u001B[39mdownload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpunkt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m stop_words \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(\u001B[43mstopwords\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwords\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m      4\u001B[0m lemmatizer \u001B[38;5;241m=\u001B[39m WordNetLemmatizer()\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess_text\u001B[39m(text):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__getattr__\u001B[1;34m(self, attr)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__bases__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLazyCorpusLoader object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__bases__\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 121\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__load\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;66;03m# __class__ to something new:\u001B[39;00m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, attr)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     84\u001B[0m             root \u001B[38;5;241m=\u001B[39m nltk\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubdir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mzip_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     85\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m:\n\u001B[1;32m---> 86\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m     88\u001B[0m \u001B[38;5;66;03m# Load the corpus.\u001B[39;00m\n\u001B[0;32m     89\u001B[0m corpus \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__reader_cls(root, \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__kwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 81\u001B[0m         root \u001B[38;5;241m=\u001B[39m \u001B[43mnltk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubdir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     83\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\data.py:583\u001B[0m, in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    581\u001B[0m sep \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m70\u001B[39m\n\u001B[0;32m    582\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 583\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[1;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mstopwords\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/stopwords\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words and lemmatize\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "tweets_df['content'] = tweets_df['content'].apply(process_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}